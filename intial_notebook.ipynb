{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain faiss-cpu sentence-transformers openai tiktoken rouge-score nltk  python-dotenv langchain-community langchain_openai rouge nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c95221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import re\n",
    "\n",
    "def load_documents_from_folder(folder_path):\n",
    "    \"\"\"Load each .txt file in the folder as a separate Document.\"\"\"\n",
    "    txt_files = Path(folder_path).glob(\"*.txt\")\n",
    "    documents = []\n",
    "    \n",
    "    for file in txt_files:\n",
    "        text = file.read_text(encoding=\"utf-8\")\n",
    "        clean_text = re.sub(r'\\s+', ' ', text.strip())  # Clean and normalize\n",
    "        doc = Document(page_content=clean_text, metadata={\"source\": file.name})\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents from folder\n",
    "folder_path = \"Dataset/ancient_greece_data\"\n",
    "documents = load_documents_from_folder(folder_path)\n",
    "\n",
    "# Create embedding model\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build FAISS index\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# SAVE TO LOCAL STORE\n",
    "vectorstore.save_local(\"faiss_index_ancient_greece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD WHEN LOADING FROM LOCAL\n",
    "vectorstore = FAISS.load_local(\"faiss_index_ancient_greece\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23450bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore.similarity_search_with_relevance_scores(query=\"who was socrates\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab10749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2538eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "contextualize_q_prompt =  \"\"\"Given above  chat history and the below  latest user question\n",
    "    which might reference context in the chat history,\n",
    "    formulate a standalone question which can be understood\n",
    "    without the chat history. Do NOT answer the question,\n",
    "    just reformulate it if needed and otherwise return it as is.\n",
    "    Below is the latest  question:\n",
    "\n",
    "    {input}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that rewrites follow-up questions into standalone questions using chat history.\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"\"\"Given the above chat history and the latest user question below,\n",
    "reformulate it into a standalone question. Do not answer the question.\n",
    "If it's already standalone, return it as is.\n",
    "\n",
    "Latest user question:\n",
    "{input}\"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", (\n",
    "                        \"\"\"You are an assistant for question-answering tasks.\n",
    "                        \"Answer this question using the provided context only.\n",
    "                        If you dont know the answer, just say 'I dont know'\n",
    "                        {context}\"\"\"\n",
    "                    )),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "contextual_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    contextual_rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f731d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": \"who is socrates\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc1235\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": \"where did he lived\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc1235\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57843a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
