{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7b5164",
   "metadata": {},
   "source": [
    "Here is a step-by-step Markdown explanation for your notebook, describing each part and its purpose:\n",
    "\n",
    "---\n",
    "\n",
    "# Conversational RAG with LangChain, FAISS, and OpenAI: Step-by-Step Explanation\n",
    "\n",
    "## 1. **Install Required Packages**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43feb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain faiss-cpu sentence-transformers openai tiktoken rouge-score nltk python-dotenv langchain-community langchain_openai rouge nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c583891",
   "metadata": {},
   "source": [
    "*Installs all necessary libraries for document loading, embeddings, vector search, LLMs, and environment management.*\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Load Environment Variables**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02331165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72938420",
   "metadata": {},
   "source": [
    "*Loads API keys and other secrets from a `.env` file, so you don’t hardcode them in your notebook.*\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Load and Preprocess Documents**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import re\n",
    "\n",
    "def load_documents_from_folder(folder_path):\n",
    "    \"\"\"Load each .txt file in the folder as a separate Document.\"\"\"\n",
    "    txt_files = Path(folder_path).glob(\"*.txt\")\n",
    "    documents = []\n",
    "    for file in txt_files:\n",
    "        text = file.read_text(encoding=\"utf-8\")\n",
    "        clean_text = re.sub(r'\\s+', ' ', text.strip())  # Clean and normalize\n",
    "        doc = Document(page_content=clean_text, metadata={\"source\": file.name})\n",
    "        documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# Load documents from folder\n",
    "folder_path = \"Dataset/ancient_greece_data\"\n",
    "documents = load_documents_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcaeb12",
   "metadata": {},
   "source": [
    "*Reads all `.txt` files from a folder, cleans the text, and wraps each as a LangChain `Document`.*\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Create Embeddings and Build FAISS Index**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "vectorstore.save_local(\"faiss_index_ancient_greece\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ecde8",
   "metadata": {},
   "source": [
    "*Converts documents to embeddings and stores them in a FAISS vector database for fast similarity search. Saves the index locally.*\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Load FAISS Index (for Reuse)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c08a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\"faiss_index_ancient_greece\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce74a4",
   "metadata": {},
   "source": [
    "*Loads the previously saved FAISS index for querying.*\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Create a Retriever**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8cc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21be830",
   "metadata": {},
   "source": [
    "*Wraps the vectorstore as a retriever object for searching relevant documents.*\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Initialize the LLM**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacda24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28996f4a",
   "metadata": {},
   "source": [
    "*Sets up the OpenAI GPT-4o-mini model for answering questions.*\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Create a History-Aware Retriever**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that rewrites follow-up questions into standalone questions using chat history.\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"\"\"Given the above chat history and the latest user question below,\n",
    "reformulate it into a standalone question. Do not answer the question.\n",
    "If it's already standalone, return it as is.\n",
    "\n",
    "Latest user question:\n",
    "{input}\"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136056ee",
   "metadata": {},
   "source": [
    "*Uses chat history to reformulate follow-up questions into standalone questions for better retrieval.*\n",
    "\n",
    "---\n",
    "\n",
    "## 9. **Create the RAG Prompt and Chain**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6747e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", (\n",
    "            \"\"\"You are an assistant for question-answering tasks.\n",
    "            Answer this question using the provided context only.\n",
    "            If you dont know the answer, just say 'I dont know'\n",
    "            {context}\"\"\"\n",
    "        )),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "contextual_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115c2f1",
   "metadata": {},
   "source": [
    "*Defines how the LLM should answer using only retrieved context. Chains the retriever and LLM together.*\n",
    "\n",
    "---\n",
    "\n",
    "## 10. **Enable Conversational Memory**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b12588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    contextual_rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cb3c0",
   "metadata": {},
   "source": [
    "*Adds chat history tracking for each session, enabling context-aware conversations.*\n",
    "\n",
    "---\n",
    "\n",
    "## 11. **Run the Conversational RAG Chain**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"who is socrates\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc1235\"}\n",
    "    },\n",
    ")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b360df8",
   "metadata": {},
   "source": [
    "*Asks a question and prints the answer, storing the conversation under a session ID.*\n",
    "\n",
    "---\n",
    "\n",
    "## 12. **Ask a Follow-up Question**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"where did he lived\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc1235\"}\n",
    "    },\n",
    ")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed9e32",
   "metadata": {},
   "source": [
    "*Asks a follow-up question. The system uses chat history to understand \"he\" refers to Socrates.*\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Load and preprocess documents** → **Embed and index with FAISS** → **Set up retriever and LLM** → **Enable conversational memory** → **Ask questions in context**.\n",
    "- This workflow enables a chatbot to answer questions about your dataset, using retrieval-augmented generation and chat history for context.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
